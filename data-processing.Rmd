---
title: "Processing EGM5 data"
author: deon
output:
  github_document:
    toc: true
    number_sections: true
---

# TL;DR workflow

(1) Get the functions from the source file 
(2) Input the irga file_name and run the functions in order.
```{r, results = FALSE}
source("functions.R")
file_name = "./sample_data/sample_data.txt"
fixed_data = fix_my_file(file_name)
clean_data = clean_my_file(fixed_data)
final_data = plot_my_data(clean_data, start_time = 31, end_time = 211, show_plots = FALSE)
final_data
```
```{r, echo = FALSE}
final_data
```

# Overview

This is a set of three functions to help with processing the EGM5 data. The functions take in the raw .txt files that are output by the machine and help get it to a usable format. The first function "fix_my_file" helps resolve the encoding issues that I encountered. The second function "clean_my_file" helps remove any unnecessary rows in the raw .txt files and produces a data.frame for downstream analysis. Finally, the last function "plot_my_data" helps to plot the data, run a linear model, and output a data.frame with some results.

We'll begin my loading the functions. This can be done by sourcing from the GitHub URL or the R script itself.

```{r}
source("functions.R")

#library(devtools)
#source_url("https://github.com/deonlum/irga-data-processing/blob/main/functions.R")

```

## A look at some data
We'll have a look at some data I collected. Note the warning message below.

```{r}
file_name = "./sample_data/sample_data.txt"
raw_data = read.table(file_name, fill = TRUE, sep = ",")
head(raw_data[,1:7]) # truncating the output for neatness
```

In this particular file, there are some encoding errors (which also causes the "embedded nul(s)" warning). I'm not sure what exactly causes it but I've had it in a few of my files. Some examples are in lines 14/15 and 644/645, where there appears to be some data that's been cut off or merged into the same line:

```{r}
raw_data[12:15,1:8]
raw_data[642:645,1:8]

```

## Resolving encoding errors
These lines can probably be completely removed without hindering the results much, but in any case, we can fix this by running fix_my_file. The function makes sure each line of data is entered properly as a new line. This doesn't remove the encoding gibberish which appears in some lines but is good enough for now - we can remove these later. 

Note that this fix won't work properly if your measurement type is different from mine (i.e., not M5 although it's simple to change it to work for anything else), or if you have other encoding issues. I've not come across others but if you do, you could also perform this step manually by editing the .txt file directly. I was originally processing files manually but it gets tedious if you have to sift through a bunch of data files to check - and could cause issues if data are errorneously deleted.

If you're not sure if your file has similar issues, you can (should?) run this anyway - it shouldn't change your data if there aren't issues. At worst, there's a (very low) possibility it'll add some empty lines in your file that get taken out with the next function. Also, the function makes sure the data is in an appropriate format for clean_my_file by giving it appropriate column names, so maybe just run this anyway.

```{r}
fixed_data = fix_my_file(file_name)
fixed_data[11:16,1:8]
fixed_data[641:646,1:8]
```

Lines 14 and 644 look fixed so we can move on to the next step.

Note that fix_my_file can be run without specifying the file name. This will open a pop up where you can select your data file to input. I've also included an option to save out the fixed file into your working directory, with "_fixed" appended to the end (so e.g., if your file is "my_data.txt" the new file will be "my_data_fixed.txt"). This just allows a more manual check to see if everything's alright.
```{r}
#fixed_data = fix_my_file()
#fixed_data = fix_my_file(, save_file = TRUE)
```

## Cleaning up data
With the errors now fixed, we can now run clean_my_file, which helps to remove irrelevant rows and prepare the data for analysis. Note that unlike fix_my_file which takes in a file name, clean_my_file takes in an already read-in .txt file. We can just input "fixed_data" here.

```{r}
clean_data = clean_my_file(fixed_data)
head(clean_data)
```

Only rows between "Start" and "End" are retained. Additionally, a measurement session variable is appended which will allow us to distinguish all the different sessions (i.e., one start/end cycle). For example, m_session = 1 was the first measurement I took that day, which was for plot 18 at 10:56AM and m_session = 2 was the second one for plot 15 at 11:03AM. This might be useful in case you have multiple measurements for the same plot (in my case, I sometimes abort and rerun the same plot, so there'll be two sessions with the same plot number).

I have sometimes faced issues with unequal numbers of "Start"s and "End"s. Again, I'm not sure why this happens, but it's easy to fix by manually editing the .txt file to delete the extra "Start"s or "End"s.

## Analysing the data
After running clean_my_file, the data can be analysed in a number of ways. This final function (plot_my_data) was written to be what I needed it to be. But you can probably just process the clean_data dataframe in any way you want.

plot_my_data has a few arguments you can input: 

* start_time and end_time can be altered to limit analysis to the desired time. For example, if I wanted to discard the first 30 seconds, I'd set start_time = 31, and if I wanted to end if 3 minutes after, I'd set end_time = 211. 

* show_plots = TRUE will allow you to run through all the individual plots, pressing enter each time to move to the next plot (hit esc to quit out of it). Informative but probably set show_plots = FALSE if you have a ton of plots. Note that it plots all measurements, ignoring the start_time/end_time arguments.

As an example, here's the final data set for the start and end times I mention earlier:
```{r}
final_data = plot_my_data(clean_data, start_time = 31, end_time = 211, show_plots = FALSE)
final_data
```

* plot_no = plot number
* delta = total change in CO2 from start to end time
* grad = gradient of the fitted linear model (note this says nothing about the fit of the line)
* session = measurement session