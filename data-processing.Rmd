---
title: "Processing EGM5 data"
output:
  github_document:
    toc: true
    number_sections: true
---

# TL;DR workflow

(1) Get the functions from the source file 
(2) Input the appropriate `file_name` and run the functions in order.
```{r, message = FALSE}
devtools::source_url("https://raw.githubusercontent.com/deonlum/irga-data-processing/main/functions.R")
```
```{r, results = FALSE}
file_name = "./sample_data/sample_data_updated.txt"
fixed_data = fix_my_file(file_name)
clean_data = clean_my_file(fixed_data)
final_data = plot_my_data(clean_data, start_time = 31, end_time = 211, show_plots = FALSE)
final_data
```
```{r, echo = FALSE}
final_data
```

# Overview

This is a set of three functions to help with processing the EGM5 data. The functions take in the raw .txt files that are output by the machine and help get it to a usable format. The first function `fix_my_file` helps resolve the encoding issues that I encountered. The second function `clean_my_file` helps remove any unnecessary rows in the raw .txt files and produces a data.frame for downstream analysis. Finally, the last function `plot_my_data` helps to plot the data, run a linear model, and output a data.frame with some results.

We'll begin by loading in the functions from `functions.R`. This can be done in a few ways: (1) sourcing from the GitHub URL (simpler, always updated; just make sure devtools is installed), (2) sourcing it from a downloaded copy in your working directory, or (3) downloading the R script and running it.

```{r, message = FALSE}
devtools::source_url("https://raw.githubusercontent.com/deonlum/irga-data-processing/main/functions.R") ## Source from URL

#source("functions.R") ## Or if sourcing from R script

```

## A look at some data
We'll have a look at some data I collected. Note the warning message below.

```{r}
file_name = "./sample_data/sample_data_updated.txt"
raw_data = read.table(file_name, fill = TRUE, sep = ",")
head(raw_data[,1:8]) # truncating the output for neatness
```

In this particular file, there are some encoding errors (which also causes the "embedded nul(s)" warning). I'm not sure what exactly causes it but I've had it in a few of my files. Some examples are in lines 14-15 and 644-645, where there appears to be some data that's been cut off or merged into the same line:

```{r}
raw_data[12:15,1:8]
raw_data[642:645,1:8]

```

## Resolving encoding errors
These lines can probably be completely removed without hindering the results much, but in any case, we can fix this by running `fix_my_file`. The function makes sure each line of data is entered properly as a new line. This doesn't remove the encoding gibberish which appears in some lines but is good enough for now - we can remove these later. 

Note that this fix won't work properly if your measurement type is different from mine (i.e., not M5... although it's simple to change this), or if you have other encoding issues. I've not come across others but if you do, you could also perform this step manually by editing the .txt file directly. I was originally processing files manually but it gets tedious if you have to sift through a bunch of data files to check - and could cause issues if data are erroneously deleted.

If you're not sure if your file has similar issues, you can (should?) run this anyway - it shouldn't change your data if there aren't issues. At worst, there's a (very low) possibility it'll add some empty lines in your file that get taken out with the next function. Also, the function makes sure the data is in an appropriate format for `clean_my_file` by giving it appropriate column names, so maybe just run this anyway.

```{r}
fixed_data = fix_my_file(file_name)
fixed_data[11:16,1:8]
fixed_data[641:646,1:8]
```

Lines 14 and 644 look fixed so we can move on to the next step.

Note that `fix_my_file` can be run without specifying the file name. This will open a pop up window that you can navigate to select your data file. I've also included an option to save out the fixed file into your working directory, with "_fixed" appended to the end (so e.g., if your file is `my_data.txt` the new file will be `my_data_fixed.txt`). This just allows for a more manual check to see if everything's alright.
```{r}
#fixed_data = fix_my_file() ## to choose a file
#fixed_data = fix_my_file(, save_file = TRUE) ## to save out a file
```

## Cleaning up data
With the errors now fixed, we can run `clean_my_file`, which helps to remove irrelevant rows and prepare the data for analysis. Note that unlike `fix_my_file` which takes in a file name, `clean_my_file` takes in an already read-in .txt file. We can just input `fixed_data` here.

```{r}
clean_data = clean_my_file(fixed_data)
head(clean_data)
```
After running the function, only rows between "Start"s and "End"s are retained. The function also prints the range of measurement duration - this should be the length of time you measured for but can vary due to issues with the data file - and tells you how many rows were removed during the clean-up.

Additionally, a measurement session variable is appended which will allow us to distinguish all the different sessions (i.e., one start/end cycle). For example, `m_session` 1 was the first measurement I took that day, which was for plot 18 at 10:56AM and `m_session` 2 was the second one for plot 15 at 11:03AM. This might be useful in case you have multiple measurements for the same plot (in my case, I sometimes abort and rerun the same plot, so there'll be two sessions with the same plot number).

I have sometimes faced issues with unequal numbers of "Start"s and "End"s from the raw file. Again, I'm not sure why this happens, but the function will throw an error if they're different. I can't be confident of when this issue pops up, so don't want to automate this bit. To resolve, you can manually edit the raw .txt file to delete the extra "Start"s or "End"s and rerun the functions.

## Analysing the data
After running `clean_my_file`, the data can be analysed in any way you want. I wrote this final function `plot_my_data` for my own purpose. It helps take a quick first look at the data so that I know whether I need to rerun a measurement on that day (e.g., if there was a leak). As such, it's quite rigid in what it does, but could still be useful for analysis depending on what you need.

`plot_my_data` has a few arguments you can input: 

* `start_time` and `end_time` can be altered to limit analysis to the desired time. For example, if I wanted to discard the first 30 seconds, I'd set `start_time = 31`, and if I wanted to end if 3 minutes after, I'd set `end_time = 211`. 

* `show_plots = TRUE` will allow you to run through all the individual plots, pressing enter each time to move to the next plot (hit esc to quit out of it). Informative but probably set `show_plots = FALSE` if you have a ton of plots. Note that it plots all measurements, ignoring the start/end time arguments.

As an example, here's the final data set for the start and end times I mention earlier:
```{r}
final_data = plot_my_data(clean_data, start_time = 31, end_time = 211, show_plots = FALSE)
final_data
```

* `plot_no` = plot number
* `delta` = total change in CO2 from start to end time
* `grad` = gradient of the fitted linear model (note this says nothing about the fit of the line)
* `r2` = R squared value of the fitted linear model
* `session` = measurement session

## Varying start and end times in plot_my_data

Start and end times can now be varied in `plot_my_data` by entering them as a vector in the start or end time argument.

```{r}
# Arbitrarily setting some start and end times
my_start_times = 1:24
my_end_times = 120-1:24

final_data = plot_my_data(clean_data, start_time = my_start_times, end_time = my_end_times, show_plots = FALSE)
final_data
```
